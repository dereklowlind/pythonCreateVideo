{
    "submissionData": {
        "author": "mto96",
        "subreddit": "devops",
        "title": "What metrics can we use to drive the things we believe in?",
        "body": "Check out this talk from GOTO Copenhagen 2019 by Roy Osherove, author of \"The Art Of Unit Testing\" and \"Elastic Leadership.Growing self-organizing teams\". You can find the link to the talk, along with the full talk abstract pasted below:\n\n[https://youtu.be/goihWvyqRow?list=PLEx5khR4g7PLHBVGOjNbevChU9DOL3Axj](https://youtu.be/goihWvyqRow?list=PLEx5khR4g7PLHBVGOjNbevChU9DOL3Axj)\n\nThey say that \"you get what you measure\", and we've all seen it happen. \"We need to get the coverage up!\" followed by people frantically writing tests that might not actually test anything. Coverage is up. Quality? Not so much. So what metrics can we use to drive the things we believe in? In this session Roy Osherove covers recommended and un-recommended metrics and how each one could drive our team towards a bleaker, or brighter future.\n\n**What will the audience learn from this talk?**\n\n* Leading vs lagging indicators and their value\n* What metrics can hurt your agility\n* What metrics push towards agility\n* Influence forces and why people behave in specific ways (and how metrics play a role)",
        "score": "56"
    },
    "commentsData": [
        {
            "level": 1,
            "author": "ikbibil86",
            "body": "My favorite metric was called something like Happy Hour Deployments... an org started measuring the % of releases that could happen during the week and allow teams to still make it to happy hour. In a couple quarters they went from grueling weekend deployments to almost all deployments on weekdays done by 4pm.",
            "isChecked": true
        },
        {
            "level": 1,
            "author": "sunzoo",
            "body": "Code coverage in particular is an interesting metric. There was a paper floating around from Microsoft that found that having good code coverage didnâ€™t mean indicate low defects, but not having it indicated bad code. Not the best metric to track, but still valuable in the must be this tall to enter sense.",
            "isChecked": true
        },
        {
            "level": 2,
            "author": "a2ur3",
            "body": "I believe it was this one:  [https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2008-11.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2008-11.pdf)",
            "isChecked": true
        },
        {
            "level": 3,
            "author": "sunzoo",
            "body": "That's the one.",
            "isChecked": true
        },
        {
            "level": 1,
            "author": "brokorus",
            "body": "Starting at what to measure is hard.\nIt's much easier if you can explicitly state goals.\n\nOkrs are good.\n\nMeasure what matters is a good book around the subject.\n\n\nIf you've hit the point of grasping for the improvement straws, I'd personally start running surveys to find what can be made better from the opinion of the devs.\n\nPerhaps less testing?\n\nIt just depends",
            "isChecked": false
        },
        {
            "level": 1,
            "author": "Chuckosaurus",
            "body": "Hopefully in the future compilers are faster and [mutation testing](https://en.wikipedia.org/wiki/Mutation_testing) becomes a more viable alternative to code coverage. Projects like [Stryker](https://stryker-mutator.io/) are awesome, but the amount of time it takes to compile your program over and over again with different defects makes it non-viable in a lot of cases. Unless your project is written in an interpreted language, then I imagine it's must faster.",
            "isChecked": false
        },
        {
            "level": 2,
            "author": "thmaje",
            "body": "I've never heard of mutation testing. I just looked it up quickly and had trouble understanding exactly what it is. Is it for measuring the code coverage?",
            "isChecked": false
        },
        {
            "level": 3,
            "author": "Chuckosaurus",
            "body": "Since code coverage doesn't represent the quality of your tests and only the percentage of lines of code which are touched while running your tests, it leaves a bit of room for improvement there.\n\nA test like this could result in 100% coverage of your function, but it doesn't prove your function functions as expected.\n\n```\ntest() {\n  myFunction();\n}\n```\n\nMutation testing statistics can be used to make better assertions about the quality and overall coverage of your tests.\n\nThe way it works is this:\n\n1. Write some code\n2. Write some unit tests\n3. Configure your mutation testing framework to enable the 'mutators' which you believe should break a good portion of your code ('mutators' are operations in which the framework will run against your codebase to temporarily alter the source code you've written. Examples of these operations are flipping negatives, changing return values, changing conditional operators, etc. - [here's a list of more](https://pitest.org/quickstart/mutators/)).\n4. Tell the framework to begin mutation testing. It will randomly apply the configured mutators against your source code (not the tests, only the source) and then run your unit tests against the mutated code. If at least one of your unit tests doesn't fail after the source code has been tampered with, then either you haven't written enough tests or tests are shit basically and shouldn't be considered code coverage.\n\nThe reason why this is computationally expensive with compiled languages is that each time you run your mutation tests, different 'mutants' will be generated randomly using your configured 'mutators', so we need to recompile every time.",
            "isChecked": false
        },
        {
            "level": 4,
            "author": "thmaje",
            "body": "Thats really neat. So basically, youre testing your tests, right? How do we test to make sure the mutators are working correctly? \\*cue inception alarm\\*",
            "isChecked": false
        },
        {
            "level": 1,
            "author": "rizzlybear",
            "body": "In my experience the best question to ask when considering if you want to measure something or not is \"is this something actionable?\"  \n\nBased on the answer to that question stuff falls into three broad buckets..  \n\nBucket 1: nope... hard pass.  \nBucket 2: No, but it would be valuable to have when investigating a problem.  \nBucket 3: yes, if this measurement is out of tolerance we need to do something about it.  \n\n\nI could go on for several hours on more granular pieces of that, but in general, if your team is chewing on that question when considering collecting a new metric, it tends to lead towards good decisions about what to do with that data.",
            "isChecked": false
        },
        {
            "level": 1,
            "author": "val-amart",
            "body": "What would you guys recommend we track if the goal is reliability (i.e. reduced downtime)?",
            "isChecked": false
        },
        {
            "level": 2,
            "author": "richardlpalmer",
            "body": "Uptime %\n\n(The interesting part of that is calculating the cost of improvement and the value (benefit) from the work.)",
            "isChecked": false
        },
        {
            "level": 2,
            "author": "EleventeenCandles",
            "body": "I'd start by tracking what is causing your downtime.  From there you can take steps to address those specific issues.\n\nFrom an overall availability perspective, there are some things that tend to lead toward higher availability, like having a well adhered-to change control process.",
            "isChecked": false
        },
        {
            "level": 1,
            "author": "HgnX",
            "body": "Question from the technical side of things, how would one go about measuring some of this automatically and aggregating this in a metricstore, preferably generating reports automatically and showing dashboards to stakeholders so they can inform themselves?",
            "isChecked": false
        }
    ]
}